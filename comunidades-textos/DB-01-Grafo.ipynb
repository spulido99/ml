{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorización de llamadas\n",
    "En este cuaderno se utliza el metodo de text-communities para realizar una clasificación no supervisada de los verbatim del nps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import re\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy.stats import powerlaw\n",
    "from gensim.models import KeyedVectors\n",
    "sns.set()\n",
    "import pyemd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import text_communities\n",
    "import importlib\n",
    "importlib.reload(text_communities)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!cat /usr/local/lib/python3.5/dist-packages/text_communities-0.1.0-py3.5.egg/text_communities/TextCommunitiesGenerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar modelo de word2vec y text communities\n",
    "Es necesario inicializar un modelo de word2vec, que se pasa como parametro para el modelo de comunidades de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  KeyedVectors.load_word2vec_format('~/voz-del-cliente/data/SBW-vectors-300-min5.txt', binary=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from text_communities.TextCommunitiesGenerator import TextCommunitiesGenerator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "generator = text_communities.TextCommunitiesGenerator.TextCommunitiesGenerator(model, n_threads=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer información de las llamadas\n",
    "Se leen las llamadas, se convierten a un dataframe y se extraen únicamente los textos de cada llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos= pd.read_csv('verbatim.csv', header=0, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos puntuación y digitos\n",
    "datos.Razon = datos.Razon.apply(lambda x: re.sub('[^á-úa-z ]+', '', str(x).lower().strip()))\n",
    "datos = datos[datos.Razon.notnull()&datos.Razon.apply(lambda x: not x == '')]\n",
    "d_frases_index = datos[['Razon']].reset_index().set_index('Razon').to_dict()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detractor = datos[datos.NPS<7]\n",
    "df_neutro = datos[(datos.NPS>6) & (datos.NPS<9)]\n",
    "df_promotor = datos[datos.NPS>8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste inicial del modelo\n",
    "Se hace un ajuste inicial del modelo de comunidades. El treshold inicial del modelo es 0.85. Este proceso se demora un tiempo significativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-533f73f44f7f>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-533f73f44f7f>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    from io\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import community\n",
    "from spellchecker import SpellChecker\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator,ClusterMixin\n",
    "import logging\n",
    "from collections import deque\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import os.path\n",
    "from google.cloud import storage\n",
    "from io import StringIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance_betweensentences(ij_sentences):\n",
    "    sentence_i, sentence_j = ij_sentences\n",
    "    dist = model.wmdistance(sentence_i, sentence_j)\n",
    "    \n",
    "    return (sentence_i, sentence_j , dist)\n",
    "\n",
    "def get_distances_matrix(vectores, vectores2=None, n_threads=4):\n",
    "\n",
    "    if vectores2 is None:\n",
    "        vectores2 = vectores\n",
    "    \n",
    "    print('''\n",
    "        ******************************************************\n",
    "        ****** Starting Parallel Matrix Calculations *********\n",
    "        ******************************************************\n",
    "    ''')\n",
    "\n",
    "       \n",
    "    comb = itertools.product(vectores, vectores2)\n",
    "    #comb = [(i, j, vectores[i], vectores[j]) for i, j in comb]\n",
    "    \n",
    "    print(' **** Combinaciones Creadas ****')\n",
    "    \n",
    "    #results = None\n",
    "\n",
    "    # No Parallel\n",
    "    # results = list(map(calc_distance_betweensentences, comb))\n",
    "\n",
    "    #Parallel\n",
    "    with Pool(n_threads) as p:\n",
    "        results = p.map(calc_distance_betweensentences, comb)\n",
    "\n",
    "    print(' **** Distancias Calculadas ****')\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_exists(bucket_name, filename):\n",
    "    client = storage.Client('pro-voz-del-cliente')\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    return blob.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(dataframe, bucket_name, filename):\n",
    "    string_io = StringIO()\n",
    "    dataframe.to_csv(string_io, index=False, encoding=\"utf-8\")\n",
    "    string_io.seek(0)\n",
    "    client = storage.Client('pro-voz-del-cliente')\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_string(string_io.getvalue(), content_type='text/csv')\n",
    "    return '{} saved'.format(filename)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "size = 600 # df_detractor.Razon.size\n",
    "step = 100\n",
    "for i in range(0, size, step):\n",
    "    for j in range(0, size, step):\n",
    "        if i >= j:\n",
    "            filename = 'edges_S{}_T{}.csv'.format(i, j)\n",
    "            print('{} already saved'.format(filename))\n",
    "            if not os.path.exists('/home/jupyter/voz-del-cliente/data/edges/'+filename):\n",
    "                textos_source = df_detractor.Razon.values[i:i+step]\n",
    "                textos_target = df_detractor.Razon.values[j:j+step]\n",
    "                distances = get_distances_matrix(textos_source, textos_target, n_threads=25)\n",
    "                df = pd.DataFrame([(d_frases_index[f1], d_frases_index[f2], d) for f1, f2, d in distances], columns=['Source', 'Target', 'Weigth'])\n",
    "                df.to_csv('/home/jupyter/voz-del-cliente/data/edges/'+filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 600 # df_detractor.Razon.size\n",
    "step = 100\n",
    "bucket_name = 'pro-voz-del-cliente-nps'\n",
    "for i in range(0, size, step):\n",
    "    for j in range(0, size, step):\n",
    "        if i >= j:\n",
    "            filename = 'edges_S{}_T{}.csv'.format(i, j)\n",
    "            print('{} already saved'.format(filename))\n",
    "            if not blob_exists(bucket_name, filename):\n",
    "                textos_source = df_detractor.Razon.values[i:i+step]\n",
    "                textos_target = df_detractor.Razon.values[j:j+step]\n",
    "                distances = get_distances_matrix(textos_source, textos_target, n_threads=25)\n",
    "                df = pd.DataFrame([(d_frases_index[f1], d_frases_index[f2], d) for f1, f2, d in distances], columns=['Source', 'Target', 'Weigth'])\n",
    "                save_file(df, bucket_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ******************************************************\n",
      "            ****** Starting Parallel Matrix Calculations *********\n",
      "            ******************************************************\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Se crea un diccionario para tener trazabilidad de los grafos generados con diferentes distancias\n",
    "diccionario_grafos = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualización parámetros del grafo\n",
    "Esta función permite actualizar el modelo con un nuevo tresh, calcula la distribucuón de los grados de libertad del grafo y ajusta la distribucion a un power law de la cual calcula elparametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_modelo(tresh):\n",
    "    generator.set_params(distances_treshold = tresh)\n",
    "    diccionario_grafos[generator.get_params()['distances_treshold']] = copy.deepcopy(generator.graph)\n",
    "    nombre_columna = 'comunidad_' + str(tresh).replace('.', '_')\n",
    "    df[nombre_columna] = generator.fit_predict(texts)\n",
    "    nx.write_gexf(generator.graph, str(tresh).replace('.', '_')+\".gexf\")\n",
    "    degree_sequence = sorted([d for n, d in diccionario_grafos[tresh].degree()], reverse=True)\n",
    "    fit = powerlaw.fit(degree_sequence)\n",
    "    alpha = fit[0]\n",
    "    print(\"Con el treshold \" + str(tresh) + \" el parametro de la distribucion es \" + str(alpha))\n",
    "    sns.distplot(degree_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.query('community5 == 42').apply(lambda x: print(x.Texto+'\\n -------------------------------------'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualizar_modelo(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
